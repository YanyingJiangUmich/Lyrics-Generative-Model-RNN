{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has three main components: an embedding layer, an LSTM layer, and a dense layer. So we basically replace the GRU player with the LSTM layer here. \n",
    "\n",
    "With the input and output layer being the same as GRU model, the LSTM layer also uses rnn_units as the number of recurrent units and is also responsible for learning the structure of input sequence, then it will return both the output sequences and the final hidden state as well as the cell state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifications made:\n",
    "\n",
    "Replaced tf.keras.layers.GRU with tf.keras.layers.LSTM\n",
    "The LSTM layer returns two states - state_h and state_c - corresponding to the hidden state and cell state respectively. We need to unpack these states from the output of the LSTM layer so we modified the if return_state block to return both state_h and state_c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWFKZaChr1Hw"
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T22:46:18.489409Z",
     "start_time": "2023-03-16T22:46:16.547920Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZ3fo50rsVbk",
    "outputId": "0bbdb64f-8dcd-4990-d071-4b1a17c2f7db",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "%matplotlib inline\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T22:46:22.788353Z",
     "start_time": "2023-03-16T22:46:22.720765Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the below command in a separate terminal for using the Tensorboard to generate tensorboard plots.\n",
    "\n",
    "```\n",
    "tensorboard --host 0.0.0.0 --logdir=./logs\n",
    "```\n",
    "\n",
    "(--host 0.0.0.0: Specifies the host IP address. Setting the host to 0.0.0.0 allows the TensorBoard server to accept connections from any IP address. This is useful when you want to access TensorBoard remotely.\n",
    "\n",
    "--logdir=./logs: Specifies the directory where TensorBoard will look for your saved files (logs) generated during TensorFlow runs. In the code above, it looks for logs in the logs directory located in the current working directory ./)\n",
    "\n",
    "You can access the board on URL: http://localhost:6006/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hB63aNFFGP3q"
   },
   "source": [
    "### Create a Function for One-Step Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The code in this cell is modified based on https://www.tensorflow.org/text/tutorials/text_generation\n",
    "import string\n",
    "import re\n",
    "\n",
    "def create_vocab(file_path):\n",
    "    # Read, then decode for py2 compat.\n",
    "    raw_text = open(file_path, 'rb').read().decode(encoding='utf-8').lower()\n",
    "    \n",
    "    # Remove parentheses and text inside parentheses and square brackets\n",
    "    # text_without_parentheses = re.sub(r'\\([^)]*\\)', '', raw_text)\n",
    "    text_without_parentheses = re.sub(r'\\([^)]*\\)|\\[[^\\]]*\\]', '', raw_text)\n",
    "\n",
    "    \n",
    "    # The unique characters in the file\n",
    "    raw_vocab = sorted(set(text_without_parentheses))\n",
    "\n",
    "    # Filter out unwanted characters \n",
    "    #Whitespace characters (including space, tab, and newline \\n)\n",
    "    allowed_chars = string.ascii_letters + string.digits + string.whitespace + \".,!?'-*\" # will keep the \"\\n\" character. keep * for censored words\n",
    "    vocab = [char for char in raw_vocab if char in allowed_chars]\n",
    "\n",
    "    # Filter the text to only include characters in allowed_chars\n",
    "    text = ''.join([char for char in text_without_parentheses if char in allowed_chars])\n",
    "\n",
    "    # length of text is the number of characters in it\n",
    "    print(f'Length of text: {len(text)} characters')\n",
    "    \n",
    "    print(f'{len(vocab)} unique characters')\n",
    "    print(f'unique characters: {vocab}')\n",
    "\n",
    "    return text, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ItOh8rkRGP3q"
   },
   "outputs": [],
   "source": [
    "### The code in this cell is modified based on https://www.tensorflow.org/text/tutorials/text_generation\n",
    "# Create a function to split the dataset\n",
    "def split_dataset(dataset, train_ratio=0.8):\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(dataset_size * train_ratio)\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    validation_dataset = dataset.skip(train_size)\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "#This function effectively splits each sequence in the dataset into an input sequence and a corresponding target sequence, which is a common preprocessing step in many natural language processing problems\n",
    "\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rnn_units = rnn_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.lstm.get_initial_state(x)\n",
    "        x, hidden_state, cell_state = self.lstm(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, (hidden_state, cell_state)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"rnn_units\": self.rnn_units,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "#https://stackoverflow.com/questions/53515547/check-perplexity-of-a-language-model\n",
    "def perplexity(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    The perplexity metric. Why isn't this part of Keras yet?!\n",
    "    https://stackoverflow.com/questions/41881308/how-to-calculate-perplexity-of-rnn-in-tensorflow\n",
    "    https://github.com/keras-team/keras/issues/8267\n",
    "    \"\"\"\n",
    "    cross_entropy = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = K.exp(cross_entropy)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8czHxgKeGP3q"
   },
   "outputs": [],
   "source": [
    "### The code in this cell is modified based on https://www.tensorflow.org/text/tutorials/text_generation\n",
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')] * len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                              return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits / self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Fcvas9vOGP3q"
   },
   "outputs": [],
   "source": [
    "### Partial of the code in this cell is modified based on https://www.tensorflow.org/text/tutorials/text_generation\n",
    "def LSTM_Generation_model(artist_name,seed_text,Epoch_size = 40, save_model = True):\n",
    "\n",
    "    file_path = 'NN_Test_Data/{}.txt'.format(artist_name)\n",
    "    text, vocab = create_vocab(file_path)\n",
    "    \n",
    "    #Now create the tf.keras.layers.StringLookup layer:\n",
    "    ids_from_chars = tf.keras.layers.StringLookup(\n",
    "        vocabulary=list(vocab), mask_token=None)\n",
    "    \n",
    "    all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "\n",
    "    ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "    chars_from_ids = tf.keras.layers.StringLookup(\n",
    "        vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "    \n",
    "    def text_from_ids(ids):\n",
    "        return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
    "\n",
    "    seq_length = 150\n",
    "    sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "    dataset = sequences.map(split_input_target)\n",
    "    \n",
    "    # Split the dataset into training and validation sets\n",
    "    train_dataset, validation_dataset = split_dataset(dataset)\n",
    "    \n",
    "    # Batch size\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "\n",
    "    train_dataset = (\n",
    "        train_dataset\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    # Batch and prefetch the validation dataset\n",
    "    validation_dataset = (\n",
    "        validation_dataset\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "    #### Build The LSTM Model\n",
    "\n",
    "    #Main Parameters:\n",
    "    # Length of the vocabulary in StringLookup Layer\n",
    "    vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "    # The embedding dimension\n",
    "    embedding_dim = 128\n",
    "\n",
    "    # Number of RNN units\n",
    "    rnn_units = 1024\n",
    "    \n",
    "    ## Number of Epochs\n",
    "    EPOCHS = Epoch_size\n",
    "\n",
    "    model = MyModel(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        rnn_units=rnn_units)\n",
    "\n",
    "\n",
    "    #### Train the Model\n",
    "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    learning_rate = 0.00040995  #default 0.001 \n",
    "    #model.compile(optimizer='adam', loss=loss, metrics=[perplexity])  # YJ: added custom metrics\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=loss, metrics=[perplexity])\n",
    "    \n",
    "    ## Define a callback to save the logs for tensorboard during training\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "\n",
    "    # Train the model with the validation dataset\n",
    "    history = model.fit(train_dataset, epochs=EPOCHS, validation_data=validation_dataset, callbacks=[tensorboard_callback])\n",
    "    \n",
    "    if save_model == True:\n",
    "        model.save(os.path.join(\"MyLSTMModel\", f\"{artist_name}_LSTM_model\"))\n",
    "            \n",
    "    one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "\n",
    "    for i, seed_text in enumerate(seed_text):\n",
    "        start = time.time()\n",
    "        states = None\n",
    "        next_char = tf.constant([seed_text])\n",
    "        result = [next_char]\n",
    "\n",
    "        n = 0\n",
    "        while True:\n",
    "            next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "            result.append(next_char)\n",
    "            n += 1\n",
    "\n",
    "            # Break the loop if the character count exceeds 1000, the next character is a newline, and the previous character is not a comma.\n",
    "            if n >= 1000 and next_char.numpy()[0].decode('utf-8') == '\\n' and result[-2].numpy()[0].decode('utf-8') != ',':\n",
    "                break\n",
    "\n",
    "        result = tf.strings.join(result)\n",
    "        end = time.time()\n",
    "        print(f\"Song {i+1}:\")\n",
    "        print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_' * 80)\n",
    "        print('\\nRun time:', end - start)\n",
    "\n",
    "        # Save the output to a text file\n",
    "        timestamp = time.strftime(\"%Y%m%d\")\n",
    "        filename = os.path.join(\"MyLSTMModel\", artist_name, f\"{artist_name}_LSTMoutput_{timestamp}_song{i+1}.txt\")\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(result[0].numpy().decode('utf-8'))\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = [\n",
    "    \"In the land of the free,\",\n",
    "    \"On a starry night,\",\n",
    "    \"Through the city streets,\",\n",
    "    \"Chasing dreams and memories,\",\n",
    "    \"As the sun goes down,\",\n",
    "    \"Lost in the rhythm of life,\",\n",
    "    \"With a heart full of hope,\",\n",
    "    \"In the shadows of skyscrapers,\",\n",
    "    \"Under the neon lights,\",  \n",
    "    \"Where the music never stops,\"  \n",
    "]\n",
    "\n",
    "top_artist = ['frank sinatra', 'elvis presley', 'dolly parton', 'lil wayne',\n",
    "              'chris brown', 'guided by voices', 'prince', 'johnny cash', 'bob dylan',\n",
    "              'george jones', 'neil young', 'bruce springsteen', 'snoop dogg',\n",
    "              'eminem', '50 cent', 'roy orbison', 'ella fitzgerald', 'taylor swift',\n",
    "              'waylon jennings', '2pac tupac shakur', 'bb king', 'bon jovi',\n",
    "              'george strait', 'madonna', 'diana ross', 'bill monroe', 'beach boys',\n",
    "              'barry manilow', 'alice cooper', 'nas', 'ray charles', 'beck']\n",
    "\n",
    "for artist_name in top_artist:\n",
    "    # Create a folder for the current artist inside the MyLSTMModel folder\n",
    "    artist_folder_path = os.path.join(\"MyLSTMModel\", artist_name)\n",
    "    os.makedirs(artist_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngsZBiPiDKoq"
   },
   "outputs": [],
   "source": [
    "#to genearate lyrics for all top artists:\n",
    "for artist in top_artist:\n",
    "    LSTM_Generation_model(artist,Epoch_size = 60,seed_text = seed_text,save_model=False)\n",
    "\n",
    "    \n",
    "#to generate lyrics for one artist: \n",
    "# artist_name = 'taylor swift'\n",
    "# LSTM_Generation_model(artist_name,Epoch_size = 40,seed_text = 'hi',save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras-tuner\n",
    "#installing the required libraries\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_tuning(artist_name,seed_text,Epoch_size =40):\n",
    "\n",
    "    file_path = 'NN_Test_Data/{}.txt'.format(artist_name)\n",
    "    text, vocab = create_vocab(file_path)\n",
    "    \n",
    "    #Now create the txf.keras.layers.StringLookup layer:\n",
    "    ids_from_chars = tf.keras.layers.StringLookup(\n",
    "        vocabulary=list(vocab), mask_token=None)\n",
    "    \n",
    "    all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "\n",
    "    ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "    chars_from_ids = tf.keras.layers.StringLookup(\n",
    "        vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "    \n",
    "    def text_from_ids(ids):\n",
    "        return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    seq_length = 150\n",
    "    sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "    dataset = sequences.map(split_input_target)\n",
    "\n",
    "    # Batch size\n",
    "    BATCH_SIZE = 64\n",
    "    BUFFER_SIZE = 10000\n",
    "    \n",
    "    dataset = (\n",
    "        dataset\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "    #Main Parameters:\n",
    "    vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "    embedding_dim = 256\n",
    "    rnn_units = 1024\n",
    "    \n",
    "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    ## Create a Random Search tuner\n",
    "    def build_model(hp):\n",
    "        model = MyModel(\n",
    "              vocab_size=vocab_size,\n",
    "              embedding_dim=hp.Choice(\"embedding_dim\", [64,128,256]),\n",
    "              rnn_units=hp.Choice(\"rnn_units\", [128,256,512,1024]))\n",
    "        \n",
    "        learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"LOG\", default=1e-3),\n",
    "        #model.compile(optimizer='adam', loss=loss, metrics=[perplexity]) \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=loss, metrics=[perplexity]) \n",
    "        return model\n",
    "    \n",
    "    objective = kt.Objective(\"perplexity\", direction=\"min\")\n",
    "    \n",
    "    keras_tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=objective,\n",
    "    max_trials=50,\n",
    "    overwrite=True,\n",
    "    directory=\"./keras_tuning\")\n",
    "    \n",
    "    keras_tuner.search(dataset,epochs=Epoch_size)\n",
    "    # Get the best hyperparameters\n",
    "    best_hyperparameters = keras_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    # Print the best hyperparameters with their values\n",
    "    for hp in best_hyperparameters.space:\n",
    "        print(f\"Best param - {hp.name}: {best_hyperparameters.get(hp.name)}\")\n",
    "        \n",
    "    return keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artist_name = 'topUS32'\n",
    "# keras_tuner = param_tuning(artist_name,seed_text = seed_text,Epoch_size = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trial 50 Complete [00h 13m 09s]\n",
    "#perplexity: 843515.125\n",
    "\n",
    "#Best perplexity So Far: 654548.0\n",
    "#Total elapsed time: 20h 30m 28s\n",
    "#Best param - embedding_dim: 128\n",
    "#Best param - rnn_units: 1024\n",
    "#Best param - learning_rate: 0.00040995319944661575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
